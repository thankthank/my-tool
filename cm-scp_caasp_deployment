#!/bin/bash
######################################################
## Values below need to be configured depending on environment
## Framework Variables : hostname, netowrk and framwork related`
DOMAIN="suse.su" 
LOCAL_REPO_DIR="/root/local_repo"
DNS_SERVER="168.126.63.1"
GATEWAY="192.168.37.254"
MGMT="caasp-lb"
MGMT_FQDN="$MGMT.$DOMAIN" #This is also FQDN of registry
MGMT_IP="192.168.37.71"
MASTER=(caasp-master1 caasp-master2 caasp-master3)
MASTER_IP=(192.168.37.36 192.168.37.37 192.168.37.38)
WORKER=(caasp-worker1 caasp-worker2 caasp-worker3 monitoring prometheus prometheus-alertmanager grafana)
WORKER_IP=(192.168.37.39 192.168.37.40 192.168.37.41 192.168.37.39 192.168.37.39 192.168.37.39 192.168.37.39)
# Hostname and IP aggregation
HOSTNAME_TOTAL=()
HOSTNAME_TOTAL[0]=$MGMT
j=${#HOSTNAME_TOTAL[@]};for i in "${MASTER[@]}";do HOSTNAME_TOTAL[$j]=$i; ((j=j+1));done;
j=${#HOSTNAME_TOTAL[@]};for i in "${WORKER[@]}";do HOSTNAME_TOTAL[$j]=$i; ((j=j+1));done;
IP_TOTAL=()
IP_TOTAL[0]=$MGMT_IP
j=${#IP_TOTAL[@]};for i in "${MASTER_IP[@]}";do IP_TOTAL[$j]=$i; ((j=j+1));done;
j=${#IP_TOTAL[@]};for i in "${WORKER_IP[@]}";do IP_TOTAL[$j]=$i; ((j=j+1));done;
CAASP_TOTAL=()
j=${#CAASP_TOTAL[@]};for i in "${MASTER[@]}";do CAASP_TOTAL[$j]=$i; ((j=j+1));done;
j=${#CAASP_TOTAL[@]};for i in "${WORKER[@]}";do CAASP_TOTAL[$j]=$i; ((j=j+1));done;
CAASP_IP_TOTAL=()
j=${#CAASP_IP_TOTAL[@]};for i in "${MASTER_IP[@]}";do CAASP_IP_TOTAL[$j]=$i; ((j=j+1));done;
j=${#CAASP_IP_TOTAL[@]};for i in "${WORKER_IP[@]}";do CAASP_IP_TOTAL[$j]=$i; ((j=j+1));done;
#echo "HOSTNAME_TOTAL : " ${#HOSTNAME_TOTAL[@]}" :" ${HOSTNAME_TOTAL[@]}

# Temporary configuration
#IP_TOTAL=(192.168.37.14)
HOSTS_TEMP=(caasp-lb) # Array for temporary run

##Function Variables
SWAP_DEV="sda2" #SWAP with thie device will be removed.
LB_IP="192.168.37.72"
NTP_CLIENT_NET="192.168.0.0/16"

#########################
## Default Configuration
## SCP_RUN Framework Variables
SCRIPT_NAME="CaaSP_deployment"
Files="temp_$(date +%y%m%d%H%M%S).sh"
Logfile="/var/log/cm-scp_caasp_deployment_$(date +%y%m%d).log"
if [[ ! -e $Logfile ]];then touch $Logfile;fi;
SCP_RUN_Target_dir='/root'
HOSTS=();

## Function Variables
PUBLIC_KEY=$(cat /home/sles/.ssh/id_rsa.pub)
CACERT=$(cat $LOCAL_REPO_DIR/cert/natgw_cert/cacert.pem)
# storage class and pvc with ceph rbd
STORAGECLASS="ceph-rbd"
CEPH_PORT="6789"
CEPH_ADMIN_IP="192.168.37.75"
CEPH_MONITOR_IP_PORT1="192.168.37.78:$CEPH_PORT"
CEPH_MONITOR_IP_PORT2="192.168.37.76:$CEPH_PORT"
CEPH_MONITOR_IP_PORT2="192.168.37.77:$CEPH_PORT"

#########################
##Generated the file
cat << EOT > /tmp/$Files
MASTER=(${MASTER[@]} )
MASTER_IP=(${MASTER_IP[@]} )
WORKER=(${WORKER[@]})
WORKER_IP=(${WORKER_IP[@]})
HOSTNAME_TOTAL=(${HOSTNAME_TOTAL[@]})
IP_TOTAL=(${IP_TOTAL[@]})

Wait () {
local DEFAULT_SEC="10"
if [[ \$1 == "" ]];then DEFAULT_SEC="10";
else DEFAULT_SEC=\$1
fi

echo "sleep \$DEFAULT_SEC seconds"
for (( i=\$DEFAULT_SEC ; i >0 ; i--   ))
do
sleep 1 ;echo -n "\$i..";
done
echo;


} 

Debug () {
echo '(('CMD'))' "\$@" | tee -a $Logfile
echo -n "Press Enter to continue this command.....";read
echo '(('OUTPUT-Started:\$(date +%FT%H:%M:%S)'))'
"\$@" | tee -a $Logfile
echo '(('OUTPUT-Done:\$(date +%FT%H:%M:%S)'))'
}

Debug_print () {
#Only print Command and no OUTPUT. It will be used commands which include terminator such as ;, >, ||
# Use single quote in Single quote
# Debug_print $'echo \'ls -al\' | grep tt '

echo '(('CMD_print'))' "\$@" | tee -a $Logfile
echo -n "Press Enter to continue this command.....";read
echo '(('OUTPUT:\$(date +%FT%H:%M:%S)'))'
}

Paste_top () {
cat \$2 >> \$1; mv \$1 \$2
}

EtcHosts_on_All () {

j=0;for i in "\${HOSTNAME_TOTAL[@]}"; do

#hostnames match but ip address is different
cat /etc/hosts | awk -v V1="\${IP_TOTAL[\$j]}" -v V2="\${HOSTNAME_TOTAL[\$j]}" \$'{
split(\$2,a,".")
if(a[1]==V2 && \$1!=V1) print "sed -i \'s/"\$0"/#"\$0"/g\' /etc/hosts"
}'|bash 
#if there is no hostname
cat /etc/hosts | grep -v ^# |  grep -w \${HOSTNAME_TOTAL[\$j]} ||echo "\${IP_TOTAL[\$j]} \$i.$DOMAIN \$i" >> /etc/hosts

(( j=j+1 ));done

}

Basic_Network_on_All () {
##disable ipv6 for all
echo "net.ipv6.conf.all.disable_ipv6 = 1" > /etc/sysctl.d/ipv6.conf

##resolv.conf for all
Debug sed -i 's/NETCONFIG_DNS_STATIC_SEARCHLIST=""/NETCONFIG_DNS_STATIC_SEARCHLIST="$DOMAIN"/g' /etc/sysconfig/network/config
Debug sed -i 's/NETCONFIG_DNS_STATIC_SERVERS=""/NETCONFIG_DNS_STATIC_SERVERS="$DNS_SERVER"/g' /etc/sysconfig/network/config

## Default route
Debug_print $'echo \'default $GATEWAY - -\' > /etc/sysconfig/network/routes'
echo 'default $GATEWAY - -' > /etc/sysconfig/network/routes
}

Basic_Configuration_on_All () {
## Install packages for all
Debug zypper --non-interactive in -t pattern enhanced_base 
Debug zypper --non-interactive in sudo wget
Debug zypper --non-interactive up --no-recommends kernel-default
## Install the package below depending on situation
#Debug zypper --non-interactive in -t pattern  yast2_basis

## Insert CA cert
cat << EOF  >  /etc/pki/trust/anchors/cacert.pem
$CACERT
EOF
# The command below will update /etc/ssl/ca-buldle.pem
Debug update-ca-certificates;

## CPU and Memory account in systemd for all
cat /etc/systemd/system.conf | grep ^'DefaultCPUAccounting=yes' || echo 'DefaultCPUAccounting=yes' >> /etc/systemd/system.conf
cat /etc/systemd/system.conf | grep ^'DefaultMemoryAccounting=yes' || echo 'DefaultMemoryAccounting=yes' >> /etc/systemd/system.conf 

## Swap off for all
#systemctl stop dev-$SWAP_DEV.swap; systemctl mask dev-$SWAP_DEV.swap;
#swapoff -a;systemctl stop swap.target;systemctl disable swap.target;
#cat /etc/fstab | grep swap > swap.tt && sed -i "s~\$(cat swap.tt)~#\$(cat swap.tt)~g" /etc/fstab

# Need to reboot or similar job for k8s to recognize Cert
sync;
sync;
}

Chrony_for_ntp_server_on_Management () {
Debug sed -i 's/! pool pool.ntp.org iburst/#! pool pool.ntp.org iburst/g' /etc/chrony.conf
Debug sed -i 's+#allow 192.168.0.0/16+allow $NTP_CLIENT_NET+g' /etc/chrony.conf
Debug sed -i 's/#local stratum 10/local stratum 10/g' /etc/chrony.conf
Debug systemctl restart chronyd
Debug systemctl enable chronyd
}

Chrony_for_ntp_client_on_CaaSP () {
Debug sed -i 's/! pool pool.ntp.org iburst/#! pool pool.ntp.org iburst/g' /etc/chrony.conf
cat /etc/chrony.conf | grep 'server $MGMT_FQDN iburst' || echo 'server $MGMT_FQDN iburst' >> /etc/chrony.conf
Debug systemctl restart chronyd
Debug systemctl enable chronyd
}

Create_a_user_for_skuba_on_All () {

## add user, sudoer and public key on all
Debug useradd -m sles
cat /etc/sudoers | grep "sles ALL=(ALL) NOPASSWD: ALL" || echo "sles ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
if [[ ! -e /home/sles/.ssh/authorized_keys ]] ; then
	mkdir -p /home/sles/.ssh ;chown sles:users /home/sles/.ssh ;chmod 700 /home/sles/.ssh;touch /home/sles/.ssh/authorized_keys; chown sles:users /home/sles/.ssh/authorized_keys; chmod 600 /home/sles/.ssh/authorized_keys
fi
## Use ssh-rsa public key of the user, sles, on management node
echo $PUBLIC_KEY >> /home/sles/.ssh/authorized_keys
}

Initialize_the_cluster_on_Management() {
Debug zypper --non-interactive in -t pattern SUSE-CaaSP-Management
## skuba cluster init --control-plane <LB IP/FQDN> <cluster name>
Debug skuba cluster init --control-plane $LB_IP my-cluster
}

Bootstrap_the_cluster_on_Management () {
cd /root/my-cluster;eval "\$(ssh-agent)";ssh-add /home/sles/.ssh/id_rsa;
Debug skuba -v 5 node bootstrap --user sles --sudo --target \${MASTER[0]}.$DOMAIN \${MASTER[0]}
}

Setup_kubectl_on_Management () {
Debug zypper --non-interactive in kubernetes-client;
mkdir -p ~/.kube;
Debug ln -s ~/my-cluster/admin.conf ~/.kube/config
}


Addtional_node_on_Management() {

echo Addtional Node
## Addtional Master node on Management
#cd /root/my-cluster;eval "\$(ssh-agent)";ssh-add /home/sles/.ssh/id_rsa;
#Debug skuba -v 5 node join --role master --user sles --sudo --target \${MASTER[2]}.$DOMAIN \${MASTER[2]}

## Addtional worker node on Management
cd /root/my-cluster;eval "\$(ssh-agent)";ssh-add /home/sles/.ssh/id_rsa;
skuba -v5  node join --role worker --user sles --sudo --target \${WORKER[2]}.$DOMAIN \${WORKER[2]}

}

Helm_Deployment_on_Management () {
# Create service account and Role Binding
Debug kubectl create serviceaccount --namespace kube-system tiller
Debug kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

Debug helm init --service-account tiller --tiller-image $MGMT_FQDN/kubernetes-helm/tiller:2.14.2
#Add SUSE chart Repo
#Debug helm repo add suse-charts https://kubernetes-charts.suse.com
#Debug helm repo add stable https://kubernetes-charts.storage.googleapis.com
}

Kubernetes_UI_on_Management () {
# Install heapster
Debug helm install --name heapster-default --namespace=kube-system localcharts/heapster --version=0.2.7 --set rbac.create=true
Wait 30
# Deploy kubernetes dashboard
Debug helm install --namespace=kube-system --name=kubernetes-dashboard localcharts/kubernetes-dashboard --version=0.6.1
Wait 30
# Dashboard access
Debug kubectl -n kube-system apply -f $LOCAL_REPO_DIR/my-tool/k8s/kubernetes-dashboard-access.yml

# Dashboard token 
Secret=\$(kubectl -n kube-system get secret | grep dashboard-token | awk '{print \$1}')
kubectl -n kube-system describe secret \$Secret | grep ^token | awk '{ print \$2 }' > ~/dashboard.token

echo 'Created a tocken as ~/dashboard.token'

}

Dashboard_proxy_on_Management () {
zypper --non-interactive in screen; 
screen -dm kubectl proxy --address 0.0.0.0 --accept-hosts '.*'

# Dashboard access using Web Browser and dashboard-token
# http://<management Ip address>:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login
}

Create_Certificate_on_Management() {

CADIR=demoCA
rm -rf ~/cmd_cert
mkdir -p ~/cmd_cert
cd ~/cmd_cert
mkdir \$CADIR
cd \$CADIR
mkdir certs crl newcerts certificate crlnumber private requests
chmod 700 private

# Copy CA key and cert
cp $LOCAL_REPO_DIR/cert/natgw_cert/cacert.pem ~/cmd_cert/\$CADIR/cacert.pem
cp $LOCAL_REPO_DIR/cert/natgw_cert/cakey.pem ~/cmd_cert/\$CADIR/private/cakey.pem

## create configuration file
##/etc/ssl/openssl.cnf is the default
cat << EOF > ~/cmd_cert/reg.cnf
HOME                    = .
RANDFILE                = \$ENV::HOME/.rnd
[ ca ]
default_ca      = CA_default            # The default ca section

[ CA_default ]
dir             = ./demoCA              # Where everything is kept
certs           = ./demoCA/certs            # Where the issued certs are kept
crl_dir         = ./demoCA/crl              # Where the issued crl are kept
database        = ./demoCA/index.txt        # database index file.
new_certs_dir   = ./demoCA/newcerts         # default place for new certs.
certificate     = ./demoCA/cacert.pem       # The CA certificate
serial          = ./demoCA/serial           # The current serial number
crlnumber       = ./demoCA/crlnumber        # the current crl number
crl             = ./demoCA/crl.pem          # The current CRL
private_key     = ./demoCA/private/cakey.pem# The private key
RANDFILE        = ./demoCA/private/.rand    # private random number file
x509_extensions = usr_cert              # The extensions to add to the cert
name_opt        = ca_default            # Subject Name options
cert_opt        = ca_default            # Certificate field options

default_days    = 3650                   # how long to certify for
default_crl_days= 30                    # how long before next CRL
default_md      = default               # use public key default MD
preserve        = no                    # keep passed DN ordering
policy          = policy_anything

[ usr_cert ]
nsComment                       = "OpenSSL Generated Certificate"
subjectKeyIdentifier=hash
authorityKeyIdentifier=keyid,issuer

[ policy_anything ]
countryName             = optional
stateOrProvinceName     = optional
localityName            = optional
organizationName        = optional
organizationalUnitName  = optional
commonName              = supplied
emailAddress            = optional

[ req ]
default_bits       = 2048
default_md         = sha512
default_keyfile    = key.pem
prompt             = no
encrypt_key        = no
distinguished_name = req_distinguished_name
req_extensions     = v3_req

[ req_distinguished_name ]
countryName            = "KR"                     # C=
stateOrProvinceName    = "Seoul"                 # ST=
localityName           = "Seoul"                 # L=
postalCode             = "11111"                 # L/postalcode=
streetAddress          = "Samsumg-ro"            # L/street=
organizationName       = "SUSE"        # O=
organizationalUnitName = "SE"          # OU=
commonName             = "$MGMT_FQDN"            # CN=
emailAddress           = "chris.chon@suse.com"  # CN/emailAddress=

[ v3_req ]
subjectAltName  = DNS:prometheus.$DOMAIN,DNS:prometheus-alertmanager.$DOMAIN,DNS:grafana.$DOMAIN # multidomain certificate

EOF

# Create private key and public key(request to be signed by CA)
cd ~/cmd_cert
openssl req -config reg.cnf -new -keyout \$CADIR/private/server_key.pem -out \$CADIR/requests/server_req.pem -newkey rsa:2048

touch \$CADIR/index.txt
echo 01 > \$CADIR/serial

# Create server certificate
openssl ca -config reg.cnf -policy policy_anything -days 3650 -out \$CADIR/certs/server_crt.pem -infiles \$CADIR/requests/server_req.pem


}

Create_MonitorCertificate_on_Management() {

CADIR=demoCA
rm -rf ~/cmd_cert
mkdir -p ~/cmd_cert
cd ~/cmd_cert
mkdir \$CADIR
cd \$CADIR
mkdir certs crl newcerts certificate crlnumber private requests
chmod 700 private

# Copy CA key and cert
cp $LOCAL_REPO_DIR/cert/natgw_cert/cacert.pem ~/cmd_cert/\$CADIR/cacert.pem
cp $LOCAL_REPO_DIR/cert/natgw_cert/cakey.pem ~/cmd_cert/\$CADIR/private/cakey.pem

## create configuration file
##/etc/ssl/openssl.cnf is the default
cat << EOF > ~/cmd_cert/reg.cnf
HOME                    = .
RANDFILE                = \$ENV::HOME/.rnd
[ ca ]
default_ca      = CA_default            # The default ca section

[ CA_default ]
dir             = ./demoCA              # Where everything is kept
certs           = ./demoCA/certs            # Where the issued certs are kept
crl_dir         = ./demoCA/crl              # Where the issued crl are kept
database        = ./demoCA/index.txt        # database index file.
new_certs_dir   = ./demoCA/newcerts         # default place for new certs.
certificate     = ./demoCA/cacert.pem       # The CA certificate
serial          = ./demoCA/serial           # The current serial number
crlnumber       = ./demoCA/crlnumber        # the current crl number
crl             = ./demoCA/crl.pem          # The current CRL
private_key     = ./demoCA/private/cakey.pem# The private key
RANDFILE        = ./demoCA/private/.rand    # private random number file
x509_extensions = usr_cert              # The extensions to add to the cert
name_opt        = ca_default            # Subject Name options
cert_opt        = ca_default            # Certificate field options

default_days    = 3650                   # how long to certify for
default_crl_days= 30                    # how long before next CRL
default_md      = default               # use public key default MD
preserve        = no                    # keep passed DN ordering
policy          = policy_anything

[ usr_cert ]
nsComment                       = "OpenSSL Generated Certificate"
subjectKeyIdentifier=hash
authorityKeyIdentifier=keyid,issuer

[ policy_anything ]
countryName             = optional
stateOrProvinceName     = optional
localityName            = optional
organizationName        = optional
organizationalUnitName  = optional
commonName              = supplied
emailAddress            = optional

[ req ]
default_bits       = 2048
default_md         = sha512
default_keyfile    = key.pem
prompt             = no
encrypt_key        = no
distinguished_name = req_distinguished_name
req_extensions     = v3_req

[ req_distinguished_name ]
countryName            = "KR"                     # C=
stateOrProvinceName    = "Seoul"                 # ST=
localityName           = "Seoul"                 # L=
postalCode             = "11111"                 # L/postalcode=
streetAddress          = "Samsumg-ro"            # L/street=
organizationName       = "SUSE"        # O=
organizationalUnitName = "SE"          # OU=
commonName             = $DOMAIN            # CN=
emailAddress           = "chris.chon@suse.com"  # CN/emailAddress=

[ v3_req ]
keyUsage = keyEncipherment, dataEncipherment
extendedKeyUsage = serverAuth
subjectAltName  = @alt_names 

[alt_names]
DNS.1 = prometheus.$DOMAIN
DNS.2 = prometheus-alertmanager.$DOMAIN
DNS.3 = grafana.$DOMAIN 

EOF

# Create private key and public key(request to be signed by CA)
cd ~/cmd_cert
openssl req -config reg.cnf -new -keyout \$CADIR/private/monitoring_key.pem -out \$CADIR/requests/monitoring_req.pem -newkey rsa:2048 -extensions 'v3_req'

touch \$CADIR/index.txt
echo 01 > \$CADIR/serial

# Create server certificate
openssl ca -config reg.cnf -policy policy_anything -days 3650 -out \$CADIR/certs/monitoring_crt.pem -infiles \$CADIR/requests/monitoring_req.pem


}


Local_registry_deployment_on_Management () {
Debug mkdir -p /etc/docker_registry/certs
Debug cp -v ~/cmd_cert/demoCA/certs/server_crt.pem /etc/docker_registry/certs/
Debug cp -v ~/cmd_cert/demoCA/private/server_key.pem /etc/docker_registry/certs/
cat << EOF > /etc/docker_registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /etc/docker/registry/certs/server_crt.pem
    key: /etc/docker/registry/certs/server_key.pem
health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
EOF
Debug mkdir -p /var/lib/docker/registry

Debug docker load -i $LOCAL_REPO_DIR/docker_images_file/registry.2.6.2.tar
Debug docker container run -d -p 443:5000 --name suse-registry -v /etc/docker_registry:/etc/docker/registry -v /var/lib/docker/registry:/var/lib/registry registry.suse.com/sles12/registry:2.6.2

#Debug docker container run -d --restart=always -p 5000:443 --name registry -v /var/lib/docker/registry:/var/lib/registry -v /var/lib/docker/certs:/certs -e REGISTRY_HTTP_ADDR=0.0.0.0:443 -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server_crt.pem -e REGISTRY_HTTP_TLS_key=/crts/server_key.pem registry.suse.com/sles12/registry:2.6.2
}

Local_registry_load_images_on_Management ()
{
local REGISTRIES=(registry.suse.com k8s.gcr.io gcr.io quay.io);

#clean up images which are loaded on this machine.
#docker images | grep -v REPOSITORY| awk '{print "docker image rm -f "\$3   }' | bash

#load and push
for i in \$(ls $LOCAL_REPO_DIR/docker_images_file/);do

Debug_print 'Result=\$(docker load -i $LOCAL_REPO_DIR/docker_images_file/\$i)'
Result=\$(docker load -i $LOCAL_REPO_DIR/docker_images_file/\$i |egrep '^Loaded image:')

ImgnamewithTar=\$(echo \$i | sed 's/\./\:/')
ImgnamewithTag=\$(echo \$ImgnamewithTar | sed 's/\.tar//g' )
ImgnameTagOnly=\${ImgnamewithTag##*:}
ImgnameTagnumberOnly=\$( echo \$ImgnameTagOnly | sed 's/v//g' )
#Below is imagename with Registry
ImgnameLoaded=\$(echo \$Result| awk -F: '{  print \$2 }' | sed 's/ //g' )

for im in "\${REGISTRIES[@]}";do
	if [[ \${ImgnameLoaded%%/*} == \$im  ]];then
		echo "========== the detected registry name: " \$im
		ImgnameOnly=\$(echo \$ImgnameLoaded | sed "s/\$im//" | sed 's+/++')
		break;

	else
		echo "=========== Registry not detected. The searched registry name " \$im;
		ImgnameOnly=\$ImgnameLoaded
	fi 
done;

echo Info: 1-ImgnamewithTar  :: 2-ImgnamewithTag  :: 3-ImgnameOnly :: 4-ImgnameTagOnly :: 5-ImgnameLoaded :: 6-ImgnameTagnumberOnly
echo Info: 1-\$ImgnamewithTar  :: 2-\$ImgnamewithTag  :: 3-\$ImgnameOnly :: 4-\$ImgnameTagOnly :: 5-\$ImgnameLoaded :: 6-\$ImgnameTagnumberOnly

# Some version number start with 'v' and some version number just start with number. So I upload both if version number in upstream start with 'v'.
Debug docker tag \$ImgnameLoaded:\$ImgnameTagOnly localhost/\$ImgnameOnly:\$ImgnameTagOnly
Debug docker tag \$ImgnameLoaded:\$ImgnameTagOnly localhost/\$ImgnameOnly:latest
Debug docker tag \$ImgnameLoaded:\$ImgnameTagOnly localhost/\$ImgnameOnly:\$ImgnameTagnumberOnly
Debug docker push localhost/\$ImgnameOnly:\$ImgnameTagOnly
Debug docker push localhost/\$ImgnameOnly:latest
Debug docker push localhost/\$ImgnameOnly:\$ImgnameTagnumberOnly

Debug docker image rm localhost/\$ImgnameOnly:\$ImgnameTagOnly
Debug docker image rm localhost/\$ImgnameOnly:latest
Debug docker image rm \$ImgnameLoaded:\$ImgnameTagOnly
Debug docker image rm localhost/\$ImgnameOnly:\$ImgnameTagnumberOnly

done

}

Local_registry_remove_on_Management () {

docker container rm -f suse-registry;
rm -rf /etc/docker_registry
rm -rf /var/lib/docker/registry

#clean up images which are loaded on this machine.
#docker images | grep -v REPOSITORY| awk '{print "docker image rm "\$3   }' | bash
}

ChangeMy-clusterToLocalRegistry_on_Management () {

Debug_print $'find ~/my-cluster -type f  | xargs grep -i registry | awk -F: \$\'{ print " sed -i \\'s+registry.suse.com+$MGMT_FQDN+g\\' "\$1"  "  }\' | bash'
find ~/my-cluster -type f  | xargs grep -i registry | awk -F: \$'{ print " sed -i \'s+registry.suse.com+$MGMT_FQDN+g\' "\$1"  "  }' | bash

}

HelmLocalChartRepoDeployment_on_Management() {

local LOCALCHARTS="localcharts"

Debug zypper --non-interactive in helm

cd $LOCAL_REPO_DIR/helm_local_repo;
Debug echo "pwd : \$PWD"
for i in \$(ls $LOCAL_REPO_DIR/helm_local_repo | egrep 'tgz\$');do
	Debug tar xvfz \$i -C $LOCAL_REPO_DIR/helm_local_repo --overwrite
	CHART=\${i%-*}
	Debug sed -i 's/www.changeme.com/$MGMT_FQDN/g' $LOCAL_REPO_DIR/helm_local_repo/\$CHART/values.yaml
	Debug tar cvfz \$i \$CHART --overwrite
	Debug rm -rf $LOCAL_REPO_DIR/helm_local_repo/\$CHART
done

# helm init --client-only
Debug helm init --client-only

cat << EOF > /root/.helm/repository/repositories.yaml
apiVersion: v1
generated: "2019-10-01T13:14:33.041010765+09:00"
repositories:
EOF

# helm repo index creation
Debug helm repo index $LOCAL_REPO_DIR/helm_local_repo --url http://$MGMT_FQDN:9001;

# Create web server for helm chart repo and register this in repo list
# Add localcharts if localcharts doesn't exists
Debug_print $'RESULT=\$(helm repo list | awk -v V1=\^\$LOCALCHARTS\' \'{if(\$1~V1) print \$1 }\')'
RESULT=\$(helm repo list | awk -v V1="^\$LOCALCHARTS" '{if(\$1~V1) print \$1 }')
Debug echo \$RESULT

if [[ ! \$RESULT =~ \$LOCALCHARTS ]]; then
	Debug screen -dm helm serve --repo-path $LOCAL_REPO_DIR/helm_local_repo --address $MGMT_FQDN:9001
	echo "sleep 10 sec until webserver up..."
	for i in {10..1};do sleep 1; echo -n \$i..;done;echo;
	Debug helm repo add \$LOCALCHARTS http://$MGMT_FQDN:9001
else 
	echo "localcharts already exists. Start helm serve, if there is no 9001 listening port";
	Debug_print $'ss -pltne | grep \':9001\' || screen -dm helm serve --repo-path $LOCAL_REPO_DIR/helm_local_repo --address $MGMT_FQDN:9001'
	ss -pltne | grep ':9001' || screen -dm helm serve --repo-path $LOCAL_REPO_DIR/helm_local_repo --address $MGMT_FQDN:9001
fi

Debug helm repo update

}


StorageclassPVCwithCephRBD_on_Management() {

Debug kubectl create namespace monitoring

#Deploy ceph-common
Debug zypper --non-interactive in ceph-common

#Locate ceph keyring and ceph.conf file in $LOCAL_REPO_DIR/ceph_conf
mkdir -p $LOCAL_REPO_DIR/ceph_conf
Debug scp -r $CEPH_ADMIN_IP:/etc/ceph/ceph* $LOCAL_REPO_DIR/ceph_conf/
Debug cm-scp $LOCAL_REPO_DIR/ceph_conf/ceph* /etc/ceph/;

CEPH_SECRET=\$(ceph auth get-key client.admin)
kubectl -n monitoring apply -f - << EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "\$(echo \$CEPH_SECRET | base64)"
EOF
kubectl apply -f - << EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "\$(echo \$CEPH_SECRET | base64)"
EOF

local CEPH_POOL="caasp-demo"
local CEPH_USER="caaspuser"
Debug ceph osd pool create \$CEPH_POOL 128 128
Debug ceph auth get-or-create client.\$CEPH_USER mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=\$CEPH_POOL" -o ceph.client.user.keyring

local USER_SECRET=\$(ceph auth get-key client.\$CEPH_USER)
kubectl -n monitoring apply -f - << EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
type: "kubernetes.io/rbd"
data:
  key: "\$(echo \$USER_SECRET | base64)"
EOF
kubectl apply -f - << EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
type: "kubernetes.io/rbd"
data:
  key: "\$(echo \$USER_SECRET | base64)"
EOF


# SotrageClass will be shared across all namespace
kubectl apply -f - << EOF
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: $STORAGECLASS
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: $CEPH_MONITOR_IP_PORT1, $CEPH_MONITOR_IP_PORT2, $CEPH_MONITOR_IP_PORT3
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: default
  pool: \$CEPH_POOL
  userId: \$CEPH_USER
  userSecretName: ceph-secret-user
EOF


#kubectl -n monitoring apply -f - << EOF
#kind: PersistentVolumeClaim
#apiVersion: v1
#metadata:
#  name: cephtest-pvc
#spec:
#  accessModes:
#  - ReadWriteOnce
#  resources:
#    requests:
#      storage: 2Gi
#EOF



#kubectl -n monitoring apply -f - << EOF
#apiVersion: v1
#kind: Pod
#metadata:
#  name: storageclass-pod
#spec:
#  containers:
#  - name: nginx-storageclass
#    image: caasp-lb.suse.su/nginx
#    volumeMounts:
#    - name: rbdvol
#      mountPath: /mnt
#      readOnly: false
#  volumes:
#  - name: rbdvol
#    persistentVolumeClaim:
#      claimName: cephtest-pvc
#
#EOF

}

LoadbalancerDeployment_on_Management () {

zypper --non-interactive in docker;
systemctl start docker; systemctl enable docker;
Debug docker container rm -f haproxy;

mkdir -p /etc/docker_haproxy
cat << EOF > /etc/docker_haproxy/haproxy.cfg
global
#Disable log below after you debug haproxy
#  log /dev/log local0 info
  daemon

defaults
  log     global
  mode    tcp
  option  tcplog
  option  redispatch
  option  tcpka
  option  dontlognull
  retries 2
  maxconn 2000
  timeout connect   5s
  timeout client    5s
  timeout server    5s
  timeout tunnel    86400s

frontend k8s-api
    bind :6443
    timeout client 5s
    default_backend k8s-api

backend k8s-api
    option tcp-check
        timeout server 5s
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

        server caasp-master1 192.168.37.36:6443 check
        server caasp-master2 192.168.37.37:6443 check
        server caasp-master3 192.168.37.38:6443 check

frontend dex
    bind :32000
    timeout client 5s
    default_backend dex

backend dex
    option tcp-check
        timeout server 5s
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

        server caasp-master1 192.168.37.36:32000 check
        server caasp-master2 192.168.37.37:32000 check
        server caasp-master3 192.168.37.38:32000 check

frontend gangway
    bind :32001
    timeout client 5s
    default_backend dex

backend gangway
    option tcp-check
        timeout server 5s
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

        server caasp-master1 192.168.37.36:32001 check
        server caasp-master2 192.168.37.37:32001 check
        server caasp-master3 192.168.37.38:32001 check

EOF

docker load -i $LOCAL_REPO_DIR/docker_images_file/haproxy.1.8.tar
docker run -d -p 6443:6443 -p 32000:32000 -p 32001:32001 --name haproxy -v /etc/docker_haproxy/haproxy.cfg:/etc/haproxy/haproxy.cfg haproxy:1.8 -f /etc/haproxy/haproxy.cfg

ip addr add $LB_IP/24 dev eth0 brd +

}

NginxIngressControllerDeployment_on_Management () {


cat << EOF > /tmp/nginx-ingress-config-values.yaml
# Enable the creation of pod security policy
podSecurityPolicy:
  enabled: true

# Create a specific service account
serviceAccount:
  create: true
  name: nginx-ingress

# Publish services on port HTTPS/30443
# These services are exposed on each node
controller:
  service:
    enableHttp: false
    type: NodePort
    nodePorts:
      https: 30443

EOF

Debug kubectl create namespace monitoring
Debug helm install --name nginx-ingress localcharts/nginx-ingress --namespace monitoring --values /tmp/nginx-ingress-config-values.yaml

}

MonitoringStackDeployment_on_Management() {

local MON_CONFIG_DIR="/etc/caasp_monitoring"
Debug kubectl create namespace monitoring


## Prometheus deployment
mkdir -p  \$MON_CONFIG_DIR
cp ~/cmd_cert/demoCA/private/monitoring_key.pem \$MON_CONFIG_DIR/
cp ~/cmd_cert/demoCA/certs/monitoring_crt.pem \$MON_CONFIG_DIR/

Debug kubectl create -n monitoring secret tls monitoring-tls --key \$MON_CONFIG_DIR/monitoring_key.pem --cert \$MON_CONFIG_DIR/monitoring_crt.pem

Debug zypper --non-interactive in apache2-utils
Debug htpasswd -c \$MON_CONFIG_DIR/auth admin

Debug kubectl create secret generic -n monitoring prometheus-basic-auth --from-file=\$MON_CONFIG_DIR/auth

cat << EOF > \$MON_CONFIG_DIR/prometheus-config-values.yaml
# Alertmanager configuration
alertmanager:
  enabled: true
  ingress:
    enabled: true
    hosts:
    -  prometheus-alertmanager.$DOMAIN
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
    tls:
      - hosts:
        - prometheus-alertmanager.$DOMAIN
        secretName: monitoring-tls
  persistentVolume:
    enabled: true
    ## Use a StorageClass
    storageClass: $STORAGECLASS
    ## Create a PersistentVolumeClaim of 2Gi
    size: 2Gi
    ## Use an existing PersistentVolumeClaim (my-pvc)
    #existingClaim: alertmanager-pvc

## Alertmanager is configured through alertmanager.yml. This file and any others
## listed in alertmanagerFiles will be mounted into the alertmanager pod.
## See configuration options https://prometheus.io/docs/alerting/configuration/
#alertmanagerFiles:
#  alertmanager.yml:

# Create a specific service account
serviceAccounts:
  nodeExporter:
    name: prometheus-node-exporter

# Allow scheduling of node-exporter on master nodes
nodeExporter:
  hostNetwork: false
  hostPID: false
  podSecurityPolicy:
    enabled: true
    annotations:
      apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
      apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
      seccomp.security.alpha.kubernetes.io/allowedProfileNames: runtime/default
      seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule

# Podsecurity for new chart
podSecurityPolicy:
  enabled: true

# Disable Pushgateway
pushgateway:
  enabled: false

# Prometheus configuration
server:
  ingress:
    enabled: true
    hosts:
    - prometheus.$DOMAIN
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
      nginx.ingress.kubernetes.io/auth-realm: "Authentication Required"
    tls:
      - hosts:
        - prometheus.$DOMAIN
        secretName: monitoring-tls
  persistentVolume:
    enabled: true
    ## Use a StorageClass
    storageClass: $STORAGECLASS
    ## Create a PersistentVolumeClaim of 8Gi
    size: 8Gi
    ## Use an existing PersistentVolumeClaim (my-pvc)
    #existingClaim: prometheus-pvc
  ## Additional Prometheus server Secret mounts
  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
  ##Remove this when adding#extraSecretMounts:
  ##Remove this when adding#- name: etcd-certs
    ##Remove this when adding#mountPath: /etc/secrets
    ##Remove this when adding#secretName: etcd-certs
    ##Remove this when adding#readOnly: true


## Prometheus is configured through prometheus.yml. This file and any others
## listed in serverFiles will be mounted into the server pod.
## See configuration options
## https://prometheus.io/docs/prometheus/latest/configuration/configuration/
#serverFiles:
#  prometheus.yml:

alertmanagerFiles:
  alertmanager.yml:
    global:
      # The smarthost and SMTP sender used for mail notifications.
      smtp_from: caasp@suse.su
      smtp_smarthost: caasp-lb.suse.su:25
     # smtp_auth_username: admin@example.com
     # smtp_auth_password: <password>
      smtp_require_tls: false

    route:
      # The labels by which incoming alerts are grouped together.
      group_by: ['node']

      # When a new group of alerts is created by an incoming alert, wait at
      # least 'group_wait' to send the initial notification.
      # This way ensures that you get multiple alerts for the same group that start
      # firing shortly after another are batched together on the first
      # notification.
      group_wait: 30s

      # When the first notification was sent, wait 'group_interval' to send a batch
      # of new alerts that started firing for that group.
      group_interval: 5m

      # If an alert has successfully been sent, wait 'repeat_interval' to
      # resend them.
      repeat_interval: 3h

      # A default receiver
      receiver: admin-example

    receivers:
    - name: 'admin-example'
      email_configs:
      - to: 'chris.chon@suse.com'

serverFiles:
  alerts: {}
  rules:
    groups:
    - name: caasp.node.rules
      rules:
      - alert: NodeIsNotReady
        expr: kube_node_status_condition{condition="Ready",status="false"} == 1
        for: 1m
        labels:
          severity: critical
        annotations:
          description: '{{ $labels.node }} is not ready'
      - alert: NodeIsOutOfDisk
        expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
        labels:
          severity: critical
        annotations:
          description: '{{ $labels.node }} has insufficient free disk space'
      - alert: NodeHasDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.node }} has insufficient available disk space'
      - alert: NodeHasInsufficientMemory
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        labels:
          severity: warning
        annotations:
          description: '{{ $labels.node }} has insufficient available memory'

EOF
Wait 1;

Debug helm install --name prometheus localcharts/prometheus --namespace monitoring --values \$MON_CONFIG_DIR/prometheus-config-values.yaml
Wait 60;

## Grafana deployment
cat << EOF > \$MON_CONFIG_DIR/grafana-datasources.yaml
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: grafana-datasources
  namespace: monitoring
  labels:
     grafana_datasource: "1"
data:
  datasource.yaml: |-
    apiVersion: 1
    deleteDatasources:
      - name: Prometheus
        orgId: 1
    datasources:
    - name: Prometheus
      type: prometheus
      url: http://prometheus-server.monitoring.svc.cluster.local:80
      access: proxy
      orgId: 1
      isDefault: true
EOF

Debug kubectl -n monitoring create -f \$MON_CONFIG_DIR/grafana-datasources.yaml

cat << EOF > \$MON_CONFIG_DIR/grafana-config-values.yaml
# Configure admin password
adminPassword: P@ssw0rd

# Ingress configuration
ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
  hosts:
    - grafana.$DOMAIN
  tls:
    - hosts:
      - grafana.$DOMAIN
      secretName: monitoring-tls

# Configure persistent storage
persistence:
  enabled: true
  accessModes:
    - ReadWriteOnce
  ## Use a StorageClass
  storageClassName: $STORAGECLASS
  ## Create a PersistentVolumeClaim of 10Gi
  size: 10Gi
  ## Use an existing PersistentVolumeClaim (my-pvc)
  #existingClaim: my-pvc

# Enable sidecar for provisioning
sidecar:
  datasources:
    enabled: true
    label: grafana_datasource
  dashboards:
    enabled: true
    label: grafana_dashboard

EOF
Wait 1;

Debug helm install --name grafana localcharts/grafana --namespace monitoring --values \$MON_CONFIG_DIR/grafana-config-values.yaml
Wait 60;

## grafana dashboard configmap copied from https://github.com/SUSE/caasp-monitoring
Debug cp $LOCAL_REPO_DIR/my-tool/k8s/grafana/*.yaml \$MON_CONFIG_DIR/

Debug kubectl -n monitoring apply -f \$MON_CONFIG_DIR/grafana-dashboards-caasp-cluster.yaml
Debug kubectl -n monitoring apply -f \$MON_CONFIG_DIR/grafana-dashboards-caasp-etcd-cluster.yaml
Debug kubectl -n monitoring apply -f \$MON_CONFIG_DIR/grafana-dashboards-caasp-namespaces.yaml
Debug kubectl -n monitoring apply -f \$MON_CONFIG_DIR/grafana-dashboards-caasp-nodes.yaml
Debug kubectl -n monitoring apply -f \$MON_CONFIG_DIR/grafana-dashboards-caasp-pods.yaml


echo;echo;
echo 'Prometheus Expression browser/API : (NodePort)https://prometheus.$DOMAIN:30443'
echo 'Alertmanager : https://prometheus-alertmanager.$DOMAIN:30443'
echo 'Grafana : https://grafana.$DOMAIN:30443'
}

MonitoringETCDCluster_on_Management() {

local MON_CONFIG_DIR="/etc/caasp_monitoring"

mkdir -p \$MON_CONFIG_DIR/etcd;
Debug scp caasp-master1:/etc/kubernetes/pki/etcd/* \$MON_CONFIG_DIR/etcd/;

Debug kubectl -n monitoring create secret generic etcd-certs --from-file=\$MON_CONFIG_DIR/etcd/ca.crt --from-file=\$MON_CONFIG_DIR/etcd/healthcheck-client.crt --from-file=\$MON_CONFIG_DIR/etcd/healthcheck-client.key

Debug sed -i 's+##Remove this when adding#++g' \$MON_CONFIG_DIR/prometheus-config-values.yaml

Debug helm upgrade prometheus localcharts/prometheus --namespace monitoring --values \$MON_CONFIG_DIR/prometheus-config-values.yaml
Wait 60

echo "Check the private IP address"
Debug kubectl get pods -n kube-system -l component=etcd -o wide

echo "Configure with 'kubectl edit -n monitoring configmap prometheus-server' as follows "
cat << EOF >&1
# Put under scrape_configs
#   scrape_configs:
    - job_name: etcd
      static_configs:
      - targets: ['${MASTER_IP[0]}:2379','${MASTER_IP[1]}:2379','${MASTER_IP[2]}:2379']
      scheme: https
      tls_config:
        ca_file: /etc/secrets/ca.crt
        cert_file: /etc/secrets/healthcheck-client.crt
        key_file: /etc/secrets/healthcheck-client.key
EOF


}


SMTPsender_on_Management() {

Debug zypper --non-interactive in postfix
Debug sed -i "s+#mynetworks = 168.100.189.0/28, 127.0.0.0/8+mynetworks = 192.168.0.0/16, 127.0.0.0/8+g" /etc/postfix/main.cf
Debug sed -i "s+inet_interfaces = localhost+inet_interfaces = all+g" /etc/postfix/main.cf
Debug systemctl enable postfix
Debug systemctl restart postfix

echo "Port 25 opened for SMTP sender"

}

RsyslogserverDeployment_on_Management () {

#mkdir -p /var/lib/docker/rsyslog
#touch /var/lib/docker/rsyslog/syslog

#mkdir -p /etc/docker_rsyslog
#cat << EOF > /etc/docker_rsyslog/remote.conf
#
## ######### Receiving Messages from Remote Hosts ##########
## TCP Syslog Server:
## provides TCP syslog reception and GSS-API (if compiled to support it)
#$ModLoad imtcp.so         # load module
###$UDPServerAddress 10.10.0.1 # force to listen on this IP only
#$InputTCPServerRun 514 # Starts a TCP server on selected port
#
## UDP Syslog Server:
##$ModLoad imudp.so         # provides UDP syslog reception
###$UDPServerAddress 10.10.0.1 # force to listen on this IP only
##$UDPServerRun 514         # start a UDP syslog server at standard port 514
#
#EOF

#Debug docker image pull $MGMT_FQDN/caasp/v4/rsyslog:8.39.0
#Debug docker run -d -p 514:514/udp -p 514:514 --name rsyslogserver -v /root/local_repo:/root/local_repo -v /etc/docker_rsyslog/remote.conf:/etc/rsyslog.d/remote.conf -v /var/lib/docker/rsyslog/syslog:/var/log/syslog $MGMT_FQDN/caasp/v4/rsyslog:8.39.0 -f /etc/rsyslog.conf


mkdir -p /var/lib/docker/rsyslog
touch /var/lib/docker/rsyslog/syslog

Debug docker image pull $MGMT_FQDN/robbert229/rsyslog:latest
Debug docker run -d -p 514:514/udp -p 514:514 --name rsyslogserver -v /root/local_repo:/root/local_repo -v /var/lib/docker/rsyslog/syslog:/var/log/syslog $MGMT_FQDN/robbert229/rsyslog:latest
}

CentralizedLoggingAgent_on_Management () {

Debug kubectl create namespace clogging

Debug helm install --namespace=clogging localcharts/log-agent-rsyslog --name rsyslog-agent --set server.host=$MGMT_IP --set server.port=514 --set server.protocol=UDP	

}

CreateSlesUser_on_Management () {

useradd -m sles; su - sles -c 'ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa';

}

PreparationBeforeDemo_on_Management () {

#Debug scp -r 192.168.37.15:/root/my-tool/cm-* /usr/bin/
echo "Change haproxy.cfg with one master node and Run below"

#LoadbalancerDeployment_on_Management 
#Debug bash /root/admin/start_service.sh


}



Test_on_Management () {
Debug echo "test"
Debug_print echo "print"
echo "print"
}

## Do not Remove Below '## Here to run'. This will be used for target setting
## Here to run

#####################################################
## Manual Preparation for cm-scp_caasp_deployment
## in the $LOCAL_REPO_DIR, my-tool, cert, RPM repo and docker_images_file need to be located
# 1. ssh-keygen and ssh-copy-id the pub_key from management to client nodes
# 2. Local_repo copy and my-tool setup
# 3. Setup repositories on client nodes

#EtcHosts_on_All
#Basic_Network_on_All
#Basic_Configuration_on_All

########################################
### Manual configuration for Hostname and static network (IP, Subnet MASK). Reboot required
# ssh caasp-master1 'echo caasp-master1 > /etc/hostname'
# ssh caasp-worker1 'echo caasp-worker1 > /etc/hostname'

#Chrony_for_ntp_server_on_Management
#Chrony_for_ntp_client_on_CaaSP

## CaaSP deployment
#CreateSlesUser_on_Management
#Create_a_user_for_skuba_on_All

#Create_Certificate_on_Management
#Local_registry_remove_on_Management 
#Local_registry_deployment_on_Management
#Local_registry_load_images_on_Management 
#HelmLocalChartRepoDeployment_on_Management
#LoadbalancerDeployment_on_Management 

## k8s cluster deployment
#PreparationBeforeDemo_on_Management
#Initialize_the_cluster_on_Management
#ChangeMy-clusterToLocalRegistry_on_Management

## From here, tasks need to be done one by one slowly!!!!!!!!!!!!!!!
#Bootstrap_the_cluster_on_Management
#Setup_kubectl_on_Management
## Need manual change on hostname for the job below!
#Addtional_node_on_Management

#Helm_Deployment_on_Management

## Monitoring Deployment. First, configure /etc/hosts configuration using EtcHosts_on_All function
## Just pick any worker node IP for monitoring.
#StorageclassPVCwithCephRBD_on_Management
#SMTPsender_on_Management
#NginxIngressControllerDeployment_on_Management
#Create_MonitorCertificate_on_Management
#MonitoringStackDeployment_on_Management
#MonitoringETCDCluster_on_Management

## Dashboard
#Kubernetes_UI_on_Management
#Dashboard_proxy_on_Management

## Centralized Logging
#RsyslogserverDeployment_on_Management
#CentralizedLoggingAgent_on_Management

## ETC

#Test_on_Management


## Do not Remove Below '## Here done'. This will be used for target setting
## Here done
EOT

######################################
## SCP_RUN_Target setting (= Mapping hostname array to SCP_RUN Target)
# It looks for targets on which the script will run using function names
# espacially, SCP_RUN_Target name will be reside at the end of function name after '_'. e.g. this_is_function_Management
# Regarding SCP_RUN_Target and hoatname array mapping, two part with "#mapping" below need to be configured.
rm -f /tmp/${SCRIPT_NAME}_HosSe.t;touch /tmp/${SCRIPT_NAME}_HosSe.t;
rm -f /tmp/${SCRIPT_NAME}_function.t; touch /tmp/${SCRIPT_NAME}_function.t;
awk -F_ -v VHOS="/tmp/${SCRIPT_NAME}_HosSe.t" -v VFUN="/tmp/${SCRIPT_NAME}_function.t;"  $'
	BEGIN{VEXE=0};{
	if($1=="## Here to run") {VEXE=1;};
	#mapping
	if(VEXE==1 && ($0~/Management *$/ || $0~/All *$/ || $0~/CaaSP *$/ || $0~/Temp *$/  ) && $1!~/^ *#/) {
		print "BA="$0";echo ${BA##*_} >> "VHOS;  
		print "BA="$0";echo ${BA} >> "VFUN;  
	};
	if($1=="## Here done") {VEXE=0;}
	};
' /usr/bin/cm-scp_caasp_deployment  | bash
SCP_RUN_Target_Num=$(awk '!a[$0]++' /tmp/${SCRIPT_NAME}_HosSe.t | wc -l) 
SCP_RUN_TARGET=$(awk '!a[$0]++' /tmp/${SCRIPT_NAME}_HosSe.t) 
SCP_RUN_FUNCTIONS=$(cat /tmp/${SCRIPT_NAME}_function.t)
NUM=$SCP_RUN_Target_Num

# Set up SCP_RUN_TARGET with the enabled SCP_RUN_TARGET above
# This part will be changed depending on script. Here, you define SCP_RUN_TARGET!
HOSTS_PRE=();
if (( NUM > 1 ));then  echo "More than one SCP RUN Targets as follows. Please speckfy only one type of target!";echo $SCP_RUN_TARGET;  exit 1;
else
	#mapping
	if [[ $SCP_RUN_TARGET == "Management" ]]; then HOSTS_PRE[0]=$MGMT_IP;           fi;
	if [[ $SCP_RUN_TARGET == "All" ]]; then HOSTS_PRE=("${IP_TOTAL[@]}");  fi;
	if [[ $SCP_RUN_TARGET == "CaaSP" ]]; then HOSTS_PRE=("${CAASP_IP_TOTAL[@]}");   fi; 
	if [[ $SCP_RUN_TARGET == "Temp" ]]; then HOSTS_PRE=("${HOSTS_TEMP[@]}");     fi; 
fi;
rm -f /tmp/${SCRIPT_NAME}_HosSe.t;
rm -f /tmp/${SCRIPT_NAME}_function.t;

#Remove duplication in HOSTS_PRE array
HOSTS=();
for i in "${HOSTS_PRE[@]}";do 
	(( non=0 ));for ii in "${HOSTS[@]}";do
		if [[ $ii != $i  ]]; then (( non=non+1 ));fi
	done
	HOSTS_COUNT=${#HOSTS[@]}
	if (( non == HOSTS_COUNT )); then HOSTS[$HOSTS_COUNT]=$i;fi
done


##Script will be SCPed and run in the remote hosts
echo;
echo "============================================================"
echo "The function and the targets are as follows."
echo "If you want to cancel, please press Ctrl+C. If not, press Enter"; echo;
echo "Functions : "$SCP_RUN_FUNCTIONS
echo '${HOSTS[@]}:  '${HOSTS[@]}
echo "============================================================"
read
for i in "${HOSTS[@]}";
do

        echo "================CMD_Started==================";
        echo $i;
        scp  /tmp/$Files $i:$SCP_RUN_Target_dir;
	ssh $i bash ${SCP_RUN_Target_dir}/${Files}
	ssh $i rm -f ${SCP_RUN_Target_dir}/${Files}
	echo "================CMD_Done==================";
        echo;

done;

rm -f /tmp/$Files;
echo "================ALL_Done==================";
