#!/bin/bash
######################################################
## Values below need to be configured depending on environment
## Framework Variables : hostname, netowrk and framwork related`
DOMAIN="suse.su" 
LOCAL_REPO_DIR="/root/local_repo"
DNS_SERVER="168.126.63.1"
GATEWAY="192.168.37.1"
MGMT="caasp-lb"
MGMT_FQDN="$MGMT.$DOMAIN" #This is also FQDN of registry
MGMT_IP="192.168.37.71"
MASTER=(caasp-master1 )
MASTER_IP=(192.168.37.30 )
WORKER=(caasp-worker1 caasp-worker2)
WORKER_IP=(192.168.37.31 192.168.37.32)
# Hostname and IP aggregation
HOSTNAME_TOTAL=()
HOSTNAME_TOTAL[0]=$MGMT
j=${#HOSTNAME_TOTAL[@]};for i in "${MASTER[@]}";do HOSTNAME_TOTAL[$j]=$i; ((j=j+1));done;
j=${#HOSTNAME_TOTAL[@]};for i in "${WORKER[@]}";do HOSTNAME_TOTAL[$j]=$i; ((j=j+1));done;
IP_TOTAL=()
IP_TOTAL[0]=$MGMT_IP
j=${#IP_TOTAL[@]};for i in "${MASTER_IP[@]}";do IP_TOTAL[$j]=$i; ((j=j+1));done;
j=${#IP_TOTAL[@]};for i in "${WORKER_IP[@]}";do IP_TOTAL[$j]=$i; ((j=j+1));done;
CAASP_TOTAL=()
j=${#CAASP_TOTAL[@]};for i in "${MASTER[@]}";do CAASP_TOTAL[$j]=$i; ((j=j+1));done;
j=${#CAASP_TOTAL[@]};for i in "${WORKER[@]}";do CAASP_TOTAL[$j]=$i; ((j=j+1));done;
CAASP_IP_TOTAL=()
j=${#CAASP_IP_TOTAL[@]};for i in "${MASTER_IP[@]}";do CAASP_IP_TOTAL[$j]=$i; ((j=j+1));done;
j=${#CAASP_IP_TOTAL[@]};for i in "${WORKER_IP[@]}";do CAASP_IP_TOTAL[$j]=$i; ((j=j+1));done;
#echo "HOSTNAME_TOTAL : " ${#HOSTNAME_TOTAL[@]}" :" ${HOSTNAME_TOTAL[@]}

# Temporary configuration
#IP_TOTAL=(192.168.37.14)
HOSTS_TEMP=(caasp-lb) # Array for temporary run

##Function Variables
SWAP_DEV="sda2" #SWAP with thie device will be removed.
LB_IP="192.168.37.72"
NTP_CLIENT_NET="192.168.0.0/16"

#########################
## Default Configuration
## SCP_RUN Framework Variables
SCRIPT_NAME="CaaSP_deployment"
Files="temp_$(date +%y%m%d%H%M%S).sh"
Logfile="/var/log/cm-scp_caasp_deployment_$(date +%y%m%d).log"
if [[ ! -e $Logfile ]];then touch $Logfile;fi;
SCP_RUN_Target_dir='/root'
HOSTS=();

## Function Variables
PUBLIC_KEY=$(cat /home/sles/.ssh/id_rsa.pub)
CACERT=$(cat $LOCAL_REPO_DIR/cert/natgw_cert/cacert.pem)
# storage class and pvc with ceph rbd
CEPH_PORT="6789"
CEPH_MONITOR_IP_PORT1="192.168.37.78:$CEPH_PORT"
CEPH_MONITOR_IP_PORT2="192.168.37.76:$CEPH_PORT"
CEPH_MONITOR_IP_PORT2="192.168.37.77:$CEPH_PORT"

#########################
##Generated the file
cat << EOT > /tmp/$Files
MASTER=(${MASTER[@]} )
MASTER_IP=(${MASTER_IP[@]} )
WORKER=(${WORKER[@]})
WORKER_IP=(${WORKER_IP[@]})
HOSTNAME_TOTAL=(${HOSTNAME_TOTAL[@]})
IP_TOTAL=(${IP_TOTAL[@]})


Debug () {
echo COMMAND:\$(date +%y%m%d_%H:%M:%S): "\$@" | tee -a $Logfile
echo OUTPUT-Started:
"\$@" | tee -a $Logfile
echo OUTPUT-Done
}

Debug_print () {
#Only print Command and no OUTPUT. It will be used commands which include terminator such as ;, >, ||
# Use single quote in Single quote
# Debug_print $'echo \'ls -al\' | grep tt '

echo COMMAND:\$(date +%y%m%d_%H:%M:%S): "\$@" | tee -a $Logfile
}

Paste_top () {
cat \$2 >> \$1; mv \$1 \$2
}

EtcHosts_on_All () {

j=0;for i in "\${HOSTNAME_TOTAL[@]}"; do

#hostnames match but ip address is different
cat /etc/hosts | awk -v V1="\${IP_TOTAL[\$j]}" -v V2="\${HOSTNAME_TOTAL[\$j]}" \$'{
split(\$2,a,".")
if(a[1]==V2 && \$1!=V1) print "sed -i \'s/"\$0"/#"\$0"/g\' /etc/hosts"
}'|bash 
#if there is no hostname
cat /etc/hosts | grep -v ^# |  grep -w \${HOSTNAME_TOTAL[\$j]} ||echo "\${IP_TOTAL[\$j]} \$i.$DOMAIN \$i" >> /etc/hosts

(( j=j+1 ));done

}

Basic_Network_on_All () {
##disable ipv6 for all
echo "net.ipv6.conf.all.disable_ipv6 = 1" > /etc/sysctl.d/ipv6.conf

##resolv.conf for all
Debug sed -i 's/NETCONFIG_DNS_STATIC_SEARCHLIST=""/NETCONFIG_DNS_STATIC_SEARCHLIST="$DOMAIN"/g' /etc/sysconfig/network/config
Debug sed -i 's/NETCONFIG_DNS_STATIC_SERVERS=""/NETCONFIG_DNS_STATIC_SERVERS="$DNS_SERVER"/g' /etc/sysconfig/network/config

## Default route
Debug_print $'echo \'default $GATEWAY - -\' > /etc/sysconfig/network/routes'
echo 'default $GATEWAY - -' > /etc/sysconfig/network/routes
}

Basic_Configuration_on_All () {
## Install packages for all
Debug zypper --non-interactive in -t pattern enhanced_base 
Debug zypper --non-interactive in sudo wget
Debug zypper --non-interactive up --no-recommends kernel-default
## Install the package below depending on situation
#Debug zypper --non-interactive in -t pattern  yast2_basis

## Insert CA cert
cat << EOF  >  /etc/pki/trust/anchors/cacert.pem
$CACERT
EOF
# The command below will update /etc/ssl/ca-buldle.pem
Debug update-ca-certificates;

## CPU and Memory account in systemd for all
cat /etc/systemd/system.conf | grep ^'DefaultCPUAccounting=yes' || echo 'DefaultCPUAccounting=yes' >> /etc/systemd/system.conf
cat /etc/systemd/system.conf | grep ^'DefaultMemoryAccounting=yes' || echo 'DefaultMemoryAccounting=yes' >> /etc/systemd/system.conf 

## Swap off for all
systemctl stop dev-$SWAP_DEV.swap; systemctl mask dev-$SWAP_DEV.swap;
swapoff -a;systemctl stop swap.target;systemctl disable swap.target;
cat /etc/fstab | grep swap > swap.tt && sed -i "s~\$(cat swap.tt)~#\$(cat swap.tt)~g" /etc/fstab

# Need to reboot or similar job for k8s to recognize Certa
sync;
sync;
}

Chrony_for_ntp_server_on_Management () {
Debug sed -i 's/! pool pool.ntp.org iburst/#! pool pool.ntp.org iburst/g' /etc/chrony.conf
Debug sed -i 's+#allow 192.168.0.0/16+allow $NTP_CLIENT_NET+g' /etc/chrony.conf
Debug sed -i 's/#local stratum 10/local stratum 10/g' /etc/chrony.conf
Debug systemctl restart chronyd
}

Chrony_for_ntp_client_CaaSP () {
Debug sed -i 's/! pool pool.ntp.org iburst/#! pool pool.ntp.org iburst/g' /etc/chrony.conf
cat /etc/chrony.conf | grep 'server $MGMT_FQDN iburst' || echo 'server $MGMT_FQDN iburst' >> /etc/chrony.conf
Debug systemctl restart chronyd
}

Create_a_user_for_skuba_on_All () {

## add user, sudoer and public key on all
Debug useradd -m sles
cat /etc/sudoers | grep "sles ALL=(ALL) NOPASSWD: ALL" || echo "sles ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
if [[ ! -e /home/sles/.ssh/authorized_keys ]] ; then
	mkdir -p /home/sles/.ssh ;chown sles:users /home/sles/.ssh ;chmod 700 /home/sles/.ssh;touch /home/sles/.ssh/authorized_keys; chown sles:users /home/sles/.ssh/authorized_keys; chmod 600 /home/sles/.ssh/authorized_keys
fi
## Use ssh-rsa public key of the user, sles, on management node
echo $PUBLIC_KEY >> /home/sles/.ssh/authorized_keys
}

Initialize_the_cluster_on_Management() {
zypper --non-interactive in -t pattern SUSE-CaaSP-Management
## skuba cluster init --control-plane <LB IP/FQDN> <cluster name>
skuba cluster init --control-plane $LB_IP my-cluster
}

Bootstrap_the_cluster_on_Management () {
cd /root/my-cluster;eval "\$(ssh-agent)";ssh-add /home/sles/.ssh/id_rsa;
Debug skuba -v 5 node bootstrap --user sles --sudo --target \${MASTER[0]}.$DOMAIN \${MASTER[0]}
}

Setup_kubectl_on_Management () {
Debug zypper --non-interactive in kubernetes-client;
Debug ln -s ~/my-cluster/admin.conf ~/.kube/config
}


Addtional_node_on_Management() {

echo Addtional Node
## Addtional Master node on Management
#cd /root/my-cluster;eval "\$(ssh-agent)";ssh-add /home/sles/.ssh/id_rsa;
#Debug skuba -v 5 node join --role master --user sles --sudo --target \${MASTER[1]}.$DOMAIN \${MASTER[1]}

## Addtional worker node on Management
#cd /root/my-cluster;eval "\$(ssh-agent)";ssh-add /home/sles/.ssh/id_rsa;
#skuba -v5  node join --role worker --user sles --sudo --target \${WORKER[0]}.$DOMAIN \${WORKER[0]}

}

Helm_Deployment_on_Management () {
# Create service account and Role Binding
Debug kubectl create serviceaccount --namespace kube-system tiller
Debug kubectl create clusterrolebinding tiller --clusterrole=cluster-admin --serviceaccount=kube-system:tiller

Debug helm init --service-account tiller --tiller-image $MGMT_FQDN/kubernetes-helm/tiller:2.14.2
#Add SUSE chart Repo
#Debug helm repo add suse-charts https://kubernetes-charts.suse.com
#Debug helm repo add stable https://kubernetes-charts.storage.googleapis.com
}

Kubernetes_UI_on_Management () {
# Install heapster
helm install --name heapster-default --namespace=kube-system localcharts/heapster --version=0.2.7 --set rbac.create=true

# Deploy kubernetes dashboard
helm install --namespace=kube-system --name=kubernetes-dashboard localcharts/kubernetes-dashboard --version=0.6.1

# Dashboard token and access
Secret=\$(kubectl -n kube-system get secret | grep dashboard-token | awk '{print \$1}')
kubectl -n kube-system describe secret \$Secret | grep ^token | awk '{ print \$2 }' > ~/dashboard.token
Debug kubectl -n kube-system apply -f $LOCAL_REPO_DIR/my-tool/k8s/kubernetes-dashboard-access.yml
}

Dashboard_proxy_on_Management () {
zypper --non-interactive in screen; 
screen -dm kubectl proxy --address 0.0.0.0 --accept-hosts '.*'

# Dashboard access using Web Browser and dashboard-token
# http://<management Ip address>:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login
}

Create_Certificate_on_Management() {

CADIR=demoCA
rm -rf ~/cmd_cert
mkdir -p ~/cmd_cert
cd ~/cmd_cert
mkdir \$CADIR
cd \$CADIR
mkdir certs crl newcerts certificate crlnumber private requests
chmod 700 private

# Copy CA key and cert
cp $LOCAL_REPO_DIR/cert/natgw_cert/cacert.pem ~/cmd_cert/\$CADIR/cacert.pem
cp $LOCAL_REPO_DIR/cert/natgw_cert/cakey.pem ~/cmd_cert/\$CADIR/private/cakey.pem

## create configuration file
##/etc/ssl/openssl.cnf is the default
cat << EOF > ~/cmd_cert/reg.cnf
HOME                    = .
RANDFILE                = \$ENV::HOME/.rnd
[ ca ]
default_ca      = CA_default            # The default ca section

[ CA_default ]
dir             = ./demoCA              # Where everything is kept
certs           = ./demoCA/certs            # Where the issued certs are kept
crl_dir         = ./demoCA/crl              # Where the issued crl are kept
database        = ./demoCA/index.txt        # database index file.
new_certs_dir   = ./demoCA/newcerts         # default place for new certs.
certificate     = ./demoCA/cacert.pem       # The CA certificate
serial          = ./demoCA/serial           # The current serial number
crlnumber       = ./demoCA/crlnumber        # the current crl number
crl             = ./demoCA/crl.pem          # The current CRL
private_key     = ./demoCA/private/cakey.pem# The private key
RANDFILE        = ./demoCA/private/.rand    # private random number file
x509_extensions = usr_cert              # The extensions to add to the cert
name_opt        = ca_default            # Subject Name options
cert_opt        = ca_default            # Certificate field options

default_days    = 3650                   # how long to certify for
default_crl_days= 30                    # how long before next CRL
default_md      = default               # use public key default MD
preserve        = no                    # keep passed DN ordering
policy          = policy_anything

[ usr_cert ]
nsComment                       = "OpenSSL Generated Certificate"
subjectKeyIdentifier=hash
authorityKeyIdentifier=keyid,issuer

[ policy_anything ]
countryName             = optional
stateOrProvinceName     = optional
localityName            = optional
organizationName        = optional
organizationalUnitName  = optional
commonName              = supplied
emailAddress            = optional

[ req ]
default_bits       = 2048
default_md         = sha512
default_keyfile    = key.pem
prompt             = no
encrypt_key        = no
distinguished_name = req_distinguished_name
req_extensions     = v3_req

[ req_distinguished_name ]
countryName            = "KR"                     # C=
stateOrProvinceName    = "Seoul"                 # ST=
localityName           = "Seoul"                 # L=
postalCode             = "11111"                 # L/postalcode=
streetAddress          = "Samsumg-ro"            # L/street=
organizationName       = "SUSE"        # O=
organizationalUnitName = "SE"          # OU=
commonName             = "$MGMT_FQDN"            # CN=
emailAddress           = "chris.chon@suse.com"  # CN/emailAddress=

[ v3_req ]
#subjectAltName  = DNS:caasp-lb.$DOMAIN,DNS:caasp-mgm.$DOMAIN # multidomain certificate

EOF

# Create private key and public key(request to be signed by CA)
cd ~/cmd_cert
openssl req -config reg.cnf -new -keyout \$CADIR/private/server_key.pem -out \$CADIR/requests/server_req.pem -newkey rsa:2048

touch \$CADIR/index.txt
echo 01 > \$CADIR/serial

# Create server certificate
openssl ca -config reg.cnf -policy policy_anything -days 3650 -out \$CADIR/certs/server_crt.pem -infiles \$CADIR/requests/server_req.pem


}

Local_registry_deployment_on_Management () {
Debug mkdir -p /etc/docker_registry/certs
Debug cp -v ~/cmd_cert/demoCA/certs/server_crt.pem /etc/docker_registry/certs/
Debug cp -v ~/cmd_cert/demoCA/private/server_key.pem /etc/docker_registry/certs/
cat << EOF > /etc/docker_registry/config.yml
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
http:
  addr: :5000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /etc/docker/registry/certs/server_crt.pem
    key: /etc/docker/registry/certs/server_key.pem
health:
  storagedriver:
    enabled: true
    interval: 10s
    threshold: 3
EOF
Debug mkdir -p /var/lib/docker/registry

Debug docker load -i $LOCAL_REPO_DIR/docker_images_file/registry.2.6.2.tar
Debug docker container run -d -p 443:5000 --name suse-registry -v /etc/docker_registry:/etc/docker/registry -v /var/lib/docker/registry:/var/lib/registry registry.suse.com/sles12/registry:2.6.2

#Debug docker container run -d --restart=always -p 5000:443 --name registry -v /var/lib/docker/registry:/var/lib/registry -v /var/lib/docker/certs:/certs -e REGISTRY_HTTP_ADDR=0.0.0.0:443 -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/server_crt.pem -e REGISTRY_HTTP_TLS_key=/crts/server_key.pem registry.suse.com/sles12/registry:2.6.2
}

Local_registry_load_images_on_Management ()
{
local REGISTRIES=(registry.suse.com k8s.gcr.io gcr.io);

#clean up images which are loaded on this machine.
#docker images | grep -v REPOSITORY| awk '{print "docker image rm -f "\$3   }' | bash

#load and push
for i in \$(ls $LOCAL_REPO_DIR/docker_images_file/);do

Debug_print 'Result=\$(docker load -i $LOCAL_REPO_DIR/docker_images_file/\$i)'
Result=\$(docker load -i $LOCAL_REPO_DIR/docker_images_file/\$i |egrep '^Loaded image:')

ImgnamewithTar=\$(echo \$i | sed 's/\./\:/')
ImgnamewithTag=\$(echo \$ImgnamewithTar | sed 's/\.tar//g' )
ImgnameTagOnly=\${ImgnamewithTag##*:}
ImgnameTagnumberOnly=\$( echo \$ImgnameTagOnly | sed 's/v//g' )
#Below is imagename with Registry
ImgnameLoaded=\$(echo \$Result| awk -F: '{  print \$2 }' | sed 's/ //g' )

for im in "\${REGISTRIES[@]}";do
	if [[ \${ImgnameLoaded%%/*} == \$im  ]];then
		echo "========== the detected registry name: " \$im
		ImgnameOnly=\$(echo \$ImgnameLoaded | sed "s/\$im//" | sed 's+/++')
		break;

	else
		echo "=========== Registry not detected. The searched registry name " \$im;
		ImgnameOnly=\$ImgnameLoaded
	fi 
done;

echo Info: 1-ImgnamewithTar  :: 2-ImgnamewithTag  :: 3-ImgnameOnly :: 4-ImgnameTagOnly :: 5-ImgnameLoaded :: 6-ImgnameTagnumberOnly
echo Info: 1-\$ImgnamewithTar  :: 2-\$ImgnamewithTag  :: 3-\$ImgnameOnly :: 4-\$ImgnameTagOnly :: 5-\$ImgnameLoaded :: 6-\$ImgnameTagnumberOnly

# Some version number start with 'v' and some version number just start with number. So I upload both if version number in upstream start with 'v'.
Debug docker tag \$ImgnameLoaded:\$ImgnameTagOnly localhost/\$ImgnameOnly:\$ImgnameTagOnly
Debug docker tag \$ImgnameLoaded:\$ImgnameTagOnly localhost/\$ImgnameOnly:latest
Debug docker tag \$ImgnameLoaded:\$ImgnameTagOnly localhost/\$ImgnameOnly:\$ImgnameTagnumberOnly
Debug docker push localhost/\$ImgnameOnly:\$ImgnameTagOnly
Debug docker push localhost/\$ImgnameOnly:latest
Debug docker push localhost/\$ImgnameOnly:\$ImgnameTagnumberOnly

Debug docker image rm localhost/\$ImgnameOnly:\$ImgnameTagOnly
Debug docker image rm localhost/\$ImgnameOnly:latest
Debug docker image rm \$ImgnameLoaded:\$ImgnameTagOnly
Debug docker image rm localhost/\$ImgnameOnly:\$ImgnameTagnumberOnly

done

}

Local_registry_remove_on_Management () {

docker container rm -f suse-registry;
rm -rf /etc/docker_registry
rm -rf /var/lib/docker/registry

#clean up images which are loaded on this machine.
#docker images | grep -v REPOSITORY| awk '{print "docker image rm "\$3   }' | bash
}

ChangeMy-clusterToLocalRegistry_on_Management () {
cd ~/my-cluster
find . -type f  | xargs grep -i registry | awk -F: \$'{ print " sed -i \'s+registry.suse.com+$MGMT_FQDN+g\' "\$1"  "  }' | bash

}

HelmLocalChartRepoDeployment_on_Management() {

local LOCALCHARTS="localcharts"

cd $LOCAL_REPO_DIR/helm_local_repo;
Debug echo "pwd : \$PWD"
for i in \$(ls $LOCAL_REPO_DIR/helm_local_repo | egrep 'tgz\$');do
	Debug tar xvfz \$i -C $LOCAL_REPO_DIR/helm_local_repo --overwrite
	CHART=\${i%-*}
	Debug sed -i 's/www.changeme.com/$MGMT_FQDN/g' $LOCAL_REPO_DIR/helm_local_repo/\$CHART/values.yaml
	Debug tar cvfz \$i \$CHART --overwrite
	Debug rm -rf $LOCAL_REPO_DIR/helm_local_repo/\$CHART
done


# helm repo index creation
Debug helm repo index $LOCAL_REPO_DIR/helm_local_repo --url http://$MGMT_FQDN:9001;

# Create web server for helm chart repo and register this in repo list
# Add localcharts if localcharts doesn't exists
Debug_print $'RESULT=\$(helm repo list | awk -v V1=\^\$LOCALCHARTS\' \'{if(\$1~V1) print \$1 }\')'
RESULT=\$(helm repo list | awk -v V1="^\$LOCALCHARTS" '{if(\$1~V1) print \$1 }')
Debug echo \$RESULT

if [[ ! \$RESULT =~ \$LOCALCHARTS ]]; then
	Debug screen -dm helm serve --repo-path $LOCAL_REPO_DIR/helm_local_repo --address $MGMT_FQDN:9001
	echo "sleep 10 sec until webserver up..."
	for i in {10..1};do sleep 1; echo -n \$i..;done;
	Debug helm repo add \$LOCALCHARTS http://$MGMT_FQDN:9001
else echo "localcharts already exists!!!";
fi

Debug helm repo update



}


StorageclassPVCwithCephRBD_on_Management() {

#Locate ceph keyring and ceph.conf file in $LOCAL_REPO_DIR/ceph_conf
Debug cm-scp $LOCAL_REPO_DIR/ceph_conf/ceph* /etc/ceph/;

CEPH_SECRET=\$(ceph auth get-key client.admin)
kubectl apply -f - << EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "\$(echo \$CEPH_SECRET | base64)"
EOF

local CEPH_POOL="caasp"
local CEPH_USER="caaspuser"
ceph osd pool create \$CEPH_POOL 124 124
ceph auth get-or-create client.\$CEPH_USER mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=\$CEPH_POOL" -o ceph.client.user.keyring

local USER_SECRET=\$(ceph auth get-key client.\$CEPH_USER)

kubectl apply -f - << EOF
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
type: "kubernetes.io/rbd"
data:
  key: "\$(echo \$USER_SECRET | base64)"
EOF

kubectl apply -f - << EOF
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: ceph-rbd
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: $CEPH_MONITOR_IP_PORT1, $CEPH_MONITOR_IP_PORT2, $CEPH_MONITOR_IP_PORT3
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: default
  pool: \$CEPH_POOL
  userId: \$CEPH_USER
  userSecretName: ceph-secret-user
EOF


kubectl apply -f - << EOF
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephtest-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
EOF

kubectl apply -f - << EOF
apiVersion: v1
kind: Pod
metadata:
  name: storageclass-pod
spec:
  containers:
  - name: nginx-storageclass
    image: caasp-lb.suse.su/nginx
    volumeMounts:
    - name: rbdvol
      mountPath: /mnt
      readOnly: false
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: cephtest-pvc

EOF

}

LoadbalancerDeployment_on_Management () {

mkdir -p /etc/docker_haproxy
cat << EOF > /etc/docker_haproxy/haproxy.cfg
global
  maxconn 256
  daemon

defaults
  log     global
  mode    http
  option  httplog
  option  dontlognull
  retries 3
  option redispatch
  maxconn 2000
  timeout connect   5000
  timeout client    50s
  timeout server    50000

frontend k8s-api
    bind :6443
    mode tcp
    option tcplog
    timeout client 300000
    default_backend k8s-api

backend k8s-api
    mode tcp
    option tcplog
    option tcp-check
        timeout server 300000
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

        server caasp-master1 192.168.37.30:6443 check

frontend dex
    bind :32000
    mode tcp
    option tcplog
    timeout client 300000
    default_backend dex

backend dex
    mode tcp
    option tcplog
    option tcp-check
        timeout server 300000
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

        server caasp-master1 192.168.37.30:32000 check

frontend gangway
    bind :32001
    mode tcp
    option tcplog
    timeout client 300000
    default_backend dex

backend gangway
    mode tcp
    option tcplog
    option tcp-check
        timeout server 300000
    balance roundrobin
    default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100

        server caasp-master1 192.168.37.30:32001 check

EOF

Debug docker load -i $LOCAL_REPO_DIR/docker_images_file/haproxy.1.8.tar
Debug docker run -d -p 6443:6443 -p 32000:32000 -p 32001:32001 --name haproxy -v /etc/docker_haproxy/haproxy.cfg:/etc/haproxy/haproxy.cfg haproxy:1.8 -f /etc/haproxy/haproxy.cfg

#Debug docker container run -d -p 443:5000 --name suse-registry -v /etc/docker_registry:/etc/docker/registry -v /var/lib/docker/registry:/var/lib/registry registry.suse.com/sles12/registry:2.6.2
}


Test_on_CaaSP () {
echo "test"
}

## Do not Remove Below '## Here to run'. This will be used for target setting
## Here to run

#####################################################
## Manual Preparation for cm-scp_caasp_deployment
## in the $LOCAL_REPO_DIR, my-tool, cert, RPM repo and docker_images_file need to be located
# 1. ssh-keygen and ssh-copy-id the pub_key from management to client nodes
# 2. Local_repo copy and my-tool setup
# 3. Setup repositories on client nodes

#EtcHosts_on_All
#Basic_Network_on_All
#Basic_Configuration_on_All

########################################
### Manual configuration for Hostname and static network (IP, Subnet MASK). Reboot required
# ssh caasp-master1 'echo caasp-master1 > /etc/hostname'
# ssh caasp-worker1 'echo caasp-worker1 > /etc/hostname'

#Chrony_for_ntp_server_on_Management
#Chrony_for_ntp_client_on_CaaSP

########################################
## Manually add user, sles, key-gen on management node
## useradd -m sles; su - sles;ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa

#LoadbalancerDeployment_on_Management 
#Create_a_user_for_skuba_on_All
#Initialize_the_cluster_on_Management
#ChangeMy-clusterToLocalRegistry_on_Management

#Create_Certificate_on_Management
#Local_registry_remove_on_Management 
#Local_registry_deployment_on_Management
#Local_registry_load_images_on_Management 

## From here, tasks need to be done one by one slowly!!!!!!!!!!!!!!!
#Bootstrap_the_cluster_on_Management
#Setup_kubectl_on_Management
## Need manual change on hostname for the job below!
#Addtional_node_on_Management

#Helm_Deployment_on_Management
#HelmLocalChartRepoDeployment_on_Management
#Kubernetes_UI_on_Management
#Dashboard_proxy_on_Management

#StorageclassPVCwithCephRBD_on_Management
#Test_on_CaaSP


## Do not Remove Below '## Here done'. This will be used for target setting
## Here done
EOT

######################################
## SCP_RUN_Target setting (= Mapping hostname array to SCP_RUN Target)
# It looks for targets on which the script will run using function names
# espacially, SCP_RUN_Target name will be reside at the end of function name after '_'. e.g. this_is_function_Management
# Regarding SCP_RUN_Target and hoatname array mapping, two part with "#mapping" below need to be configured.
rm -f /tmp/${SCRIPT_NAME}_HosSe.t;touch /tmp/${SCRIPT_NAME}_HosSe.t;
rm -f /tmp/${SCRIPT_NAME}_function.t; touch /tmp/${SCRIPT_NAME}_function.t;
awk -F_ -v VHOS="/tmp/${SCRIPT_NAME}_HosSe.t" -v VFUN="/tmp/${SCRIPT_NAME}_function.t;"  $'
	BEGIN{VEXE=0};{
	if($1=="## Here to run") {VEXE=1;};
	#mapping
	if(VEXE==1 && ($0~/Management *$/ || $0~/All *$/ || $0~/CaaSP *$/ || $0~/Temp *$/  ) && $1!~/^ *#/) {
		print "BA="$0";echo ${BA##*_} >> "VHOS;  
		print "BA="$0";echo ${BA} >> "VFUN;  
	};
	if($1=="## Here done") {VEXE=0;}
	};
' /usr/bin/cm-scp_caasp_deployment  | bash
SCP_RUN_Target_Num=$(awk '!a[$0]++' /tmp/${SCRIPT_NAME}_HosSe.t | wc -l) 
SCP_RUN_TARGET=$(awk '!a[$0]++' /tmp/${SCRIPT_NAME}_HosSe.t) 
SCP_RUN_FUNCTIONS=$(cat /tmp/${SCRIPT_NAME}_function.t)
NUM=$SCP_RUN_Target_Num

# Set up SCP_RUN_TARGET with the enabled SCP_RUN_TARGET above
# This part will be changed depending on script. Here, you define SCP_RUN_TARGET!
if (( NUM > 1 ));then  echo "More than one SCP RUN Targets as follows. Please speckfy only one type of target!";echo $SCP_RUN_TARGET;  exit 1;
else
	#mapping
	if [[ $SCP_RUN_TARGET == "Management" ]]; then HOSTS[0]=$MGMT_IP;           fi;
	if [[ $SCP_RUN_TARGET == "All" ]]; then HOSTS=("${IP_TOTAL[@]}");  fi;
	if [[ $SCP_RUN_TARGET == "CaaSP" ]]; then HOSTS=("${CAASP_IP_TOTAL[@]}");   fi; 
	if [[ $SCP_RUN_TARGET == "Temp" ]]; then HOSTS=("${HOSTS_TEMP[@]}");     fi; 
fi;
rm -f /tmp/${SCRIPT_NAME}_HosSe.t;
rm -f /tmp/${SCRIPT_NAME}_function.t;

##Script will be SCPed and run in the remote hosts
echo;
echo "============================================================"
echo "The function and the targets are as follows."
echo "If you want to cancel, please press Ctrl+C. If not, press Enter"; echo;
echo "Functions : "$SCP_RUN_FUNCTIONS
echo '${HOSTS[@]}:  '${HOSTS[@]}
echo "============================================================"
read
for i in "${HOSTS[@]}";
do

        echo "================CMD_Started==================";
        echo $i;
        scp  /tmp/$Files $i:$SCP_RUN_Target_dir;
	ssh $i bash ${SCP_RUN_Target_dir}/${Files}
#	ssh $i rm -f ${SCP_RUN_Target_dir}/${Files}
	echo "================CMD_Done==================";
        echo;

done;

rm -f /tmp/$Files;
echo "================ALL_Done==================";
